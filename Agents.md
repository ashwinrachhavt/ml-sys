Youâ€™re basically asking for a **FastAPI-first MLOps template** that you can reuse for any tabular ML project. Letâ€™s design one that hits all 8 of your requirements.

Below is a concrete **repo structure + responsibilities**; after that Iâ€™ll map each requirement to the right modules.

---

## ğŸ§± Top-level layout

```text
mlops_fastapi_service/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml           # or setup.py + requirements.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml           # pytest + lint + type-check
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # optional, mainly for local dev
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ artifacts/           # local models, profiles, etc (or S3)
â”‚
â”œâ”€â”€ mlflow/                  # tracking store if run locally
â”‚   â””â”€â”€ mlruns/
â”‚
â”œâ”€â”€ app/                     # FastAPI application package (recommended pattern) :contentReference[oaicite:0]{index=0}
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # create FastAPI() and include routers
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                # â€œinfrastructureâ€: settings, logging, deps
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py        # Pydantic Settings: paths, mlflow URI, etc.
â”‚   â”‚   â”œâ”€â”€ logging.py       # struct log config
â”‚   â”‚   â””â”€â”€ mlflow_utils.py  # helpers to start runs, log artifacts
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                 # all FastAPI routers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ deps.py          # dependencies (DB / model loader / auth)
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ eda.py       # /eda/profile
â”‚   â”‚   â”‚   â”œâ”€â”€ training.py  # /admin/train, /admin/models
â”‚   â”‚   â”‚   â”œâ”€â”€ inference.py # /predict, /predict/batch
â”‚   â”‚   â”‚   â””â”€â”€ health.py    # /health, /metrics
â”‚   â”‚   â””â”€â”€ routers.py       # include_router(...) wiring
â”‚   â”‚
â”‚   â”œâ”€â”€ schemas/             # Pydantic models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ eda.py           # request/response for profile endpoint
â”‚   â”‚   â”œâ”€â”€ training.py      # train request (model_name, params, run_name)
â”‚   â”‚   â””â”€â”€ inference.py     # ProspectFeatures, Prediction, BatchPrediction
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                # data loading & validation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_loader.py   # abstract DataLoader
â”‚   â”‚   â”œâ”€â”€ pandas_loader.py # current implementation
â”‚   â”‚   â”œâ”€â”€ validators.py    # schema checks, type checks (pydantic + pandera)
â”‚   â”‚   â””â”€â”€ transforms.py    # small reusable transforms (e.g. recency calc)
â”‚   â”‚
â”‚   â”œâ”€â”€ features/            # feature pipelines (offline)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_transformer.py
â”‚   â”‚   â”œâ”€â”€ feature_builder.py  # your final feature-engineering logic
â”‚   â”‚   â”œâ”€â”€ pipelines.py        # compose loaders + builder into a DAG-ish flow
â”‚   â”‚   â””â”€â”€ store.py            # simple feature â€œregistryâ€ / versioning
â”‚   â”‚
â”‚   â”œâ”€â”€ ml/                  # modeling code (training + inference)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_model.py       # unified interface: fit/predict/predict_proba
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ logistic.py
â”‚   â”‚   â”‚   â”œâ”€â”€ xgboost.py
â”‚   â”‚   â”‚   â”œâ”€â”€ catboost.py
â”‚   â”‚   â”‚   â””â”€â”€ lgbm.py
â”‚   â”‚   â”œâ”€â”€ trainer.py          # training orchestration, MLflow logging
â”‚   â”‚   â”œâ”€â”€ registry.py         # load/save best model from MLflow or disk
â”‚   â”‚   â””â”€â”€ evaluation.py       # metrics, cross-validation helpers
â”‚   â”‚
â”‚   â”œâ”€â”€ serving/             # runtime prediction logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ predictor.py      # load preprocessor + model; single/batch predict
â”‚   â”‚   â”œâ”€â”€ batch_scorer.py   # offline scoring job (can be called via CLI)
â”‚   â”‚   â””â”€â”€ monitoring.py     # log prediction metrics, request timing, etc.
â”‚   â”‚
â”‚   â””â”€â”€ monitoring/          # observability hooks
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ prometheus.py     # Prometheus metrics endpoint & middleware
â”‚       â””â”€â”€ drift.py          # simple PSI / KS tests for input drift, etc.
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_features.py     # CLI wrapper: run feature pipeline once
â”‚   â”œâ”€â”€ train.py              # CLI training entrypoint (calls app.ml.trainer)
â”‚   â”œâ”€â”€ evaluate.py           # offline eval / comparison against test set
â”‚   â””â”€â”€ serve.py              # uvicorn entrypoint (optional)
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_data_loaders.py
    â”œâ”€â”€ test_feature_builder.py
    â”œâ”€â”€ test_trainer.py
    â”œâ”€â”€ test_inference_api.py
    â””â”€â”€ fixtures/             # sample CSVs, stub models, etc.
```

This mirrors patterns from common FastAPI project-structure guides and templates (separating `core`, `api`, `schemas`, `services/ml` etc.) while staying simple enough to navigate. ([Medium][1])

---

## ğŸ”Œ How each of your requirements fits

### 1. EDA profile report endpoint (pandas/ydata-profiling)

* **Files:**

  * `app/api/v1/eda.py` â€“ FastAPI router for `/eda/profile`
  * `app/schemas/eda.py` â€“ request/response models
* **Implementation idea:**

  * Accept a file upload (`UploadFile`) or S3 URL.
  * Use `pandas.read_csv(...)` then `ydata_profiling.ProfileReport(df, minimal=True)` to generate HTML. (Pandas Profiling is now `ydata-profiling` but the API is the same. ([DataCamp][2]))
  * Store the HTML in:

    * local `data/artifacts/profiles/` **and**
    * log it as an artifact to MLflow (via `mlflow.log_text(html, "eda/profile_<timestamp>.html")`).
  * Return either:

    * A direct HTMLResponse, or
    * A JSON response with a link to the saved artifact.

This endpoint becomes a generic â€œEDA microserviceâ€ â€“ any CSV â†’ quick profile.

---

### 2. Composable Dataloaders & feature transforms

* **Files:**

  * `app/data/base_loader.py` â€“ abstract `load_raw()` method.
  * `app/data/pandas_loader.py` â€“ current implementation using CSVs.
  * `app/features/feature_builder.py` â€“ merges tables, engineers features.
  * `app/features/pipelines.py` â€“ orchestrates loader + builder into one flow.

Pattern:

```python
# app/data/base_loader.py
class BaseDataLoader(ABC):
    @abstractmethod
    def load_raw(self) -> Dict[str, pd.DataFrame]:
        ...

# app/data/pandas_loader.py
class PandasCSVLoader(BaseDataLoader):
    def load_raw(self) -> Dict[str, pd.DataFrame]:
        # read customers.csv, noncustomers.csv, usage_actions.csv
        ...

# app/features/pipelines.py
def build_training_features(loader: BaseDataLoader) -> FeatureBuildResult:
    raw = loader.load_raw()
    return FeatureBuilder().build(**raw)
```

Later, when a real ETL exists, you just add a new loader (e.g. `SnowflakeLoader`) implementing the same interface.

---

### 3. Training pipeline inside FastAPI (and CLI)

* **Files:**

  * `app/ml/trainer.py`
  * `app/ml/registry.py`
  * `app/api/v1/training.py`
  * `scripts/train.py`

**Core idea:**

* `trainer.py` exposes a function like:

  ```python
  def train_and_register_model(
      model_name: str,
      params: dict,
      experiment_name: str,
  ) -> TrainedModelInfo:
      # 1. build features (or reuse pre-saved features)
      # 2. run KFold/holdout training
      # 3. log metrics, params, plots to MLflow
      # 4. register â€œbestâ€ run in MLflow Model Registry or local registry
      ...
  ```

* FastAPI **admin endpoint** `/admin/train` (with auth!) calls that function to kick off training jobs.

* `scripts/train.py` is a thin CLI wrapper that calls the same trainer, so notebooks / CLI / API all share the **same training logic**.

MLflow orchestration using the same patterns you already implemented (model signature + input_example, log_figure/log_text instead of writing local files). ([Medium][3])

---

### 4. Online and batch inference endpoints

* **Files:**

  * `app/serving/predictor.py` â€“ core logic: load preprocessor + model; `predict_one` and `predict_batch`.
  * `app/api/v1/inference.py` â€“ FastAPI router.
  * `app/schemas/inference.py` â€“ `ProspectFeatures`, `Prediction`, `BatchPredictionRequest`, etc.
  * `app/serving/batch_scorer.py` â€“ offline scoring job used by `/predict/batch` and scripts.

**Flow (online):**

1. `/predict` receives JSON matching `ProspectFeatures` (Pydantic).
2. API converts to pandas/DataFrame or dict of features.
3. `predictor.load_model()` returns (cached) preprocessor + model (from MLflow or local artifact).
4. `predictor.predict_one(features)` returns score + label.
5. Endpoint logs prediction latency and basic stats to Prometheus + MLflow (for monitoring).

**Flow (batch):**

* `/predict/batch` accepts:

  * uploaded CSV, or
  * a list of `ProspectFeatures` items.
* Uses `predictor.predict_batch(df)` â†’ array of probabilities + labels.
* Optionally stores full prediction CSV to `data/artifacts/predictions/` and logs as MLflow artifact.

---

### 5. MLflow for training *and* inference metrics

You already have the training side; you can:

* Set the tracking URI & experiment in `app/core/mlflow_utils.py`.
* For **training**, `trainer.py` starts runs, logs params, metrics, artifacts.
* For **inference**, you can:

  * either reuse a â€œmonitoringâ€ experiment (e.g. `prospect_conversion_inference`),
  * or push aggregated metrics (like rolling AUC once ground truth arrives).

MLflow docs describe model packaging + deployment, and in v3 they even include a built-in FastAPI inference server; your version is a customized version of that inside your own app. ([MLflow][4])

---

### 6â€“7. Docker, Docker Compose, monitoring

* **Files:**

  * `Dockerfile` â€“ multi-stage build (install deps, copy app, run uvicorn).
  * `docker-compose.yml` â€“ FastAPI service + MLflow server + Prometheus + Grafana.
  * `monitoring/prometheus.yml`, `monitoring/grafana_dashboard.json`.

Use FastAPIâ€™s recommended Docker patterns (non-root user, gunicorn/uvicorn worker, health checks) and general best practices like small base image, env-driven config. ([FastAPI][5])

Typical Dockerfile skeleton:

```dockerfile
FROM python:3.11-slim AS base

WORKDIR /app
ENV PYTHONUNBUFFERED=1

COPY pyproject.toml requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY app ./app
COPY scripts ./scripts

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

`docker-compose.yml` then wires:

* `web` â†’ FastAPI container
* `mlflow` â†’ MLflow tracking server (with mounted volume for `mlruns/`)
* `prometheus` & `grafana` â†’ scrape `/metrics` from `web`

---

### 8. GitHub Actions CI + pytest + stubs

* **Files:**

  * `.github/workflows/ci.yml`
  * `tests/` with fixtures and unit tests.

CI pipeline ideas:

* **Jobs:**

  * `lint` â€“ run `ruff` or `flake8`
  * `type-check` â€“ `mypy app`
  * `test` â€“ `pytest tests/`
* Pin Python version matrix if you want (e.g. 3.10, 3.11).
* Use fixtures + stub objects:

  * fake DataLoader that returns a tiny DataFrame
  * fake Model that returns constant probabilities
  * so tests donâ€™t depend on heavy ML frameworks.

Example minimal `ci.yml` outline:

```yaml
name: CI

on:
  push:
    branches: [ main ]
  pull_request:

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - run: pip install -r requirements.txt
      - run: pytest -q
```

---

## Why this structure works for what you want

* **Composable**: data loaders, feature builders, and models all hang off abstract base classes â€“ easy to swap implementations without touching API code.
* **Single source of truth**: training logic in `app/ml/trainer.py` is used by notebooks, CLI, and admin endpoints.
* **FastAPI-centric**: everything lives in `app/` (as per common FastAPI templates) with clear separation between `api`, `schemas`, `core`, and `ml/serving`. ([GitHub][6])
* **Production-minded**: Docker, MLflow, Prometheus/Grafana, CI â€“ all wired from day one.

If youâ€™d like, next step we can:

* Pick **one** model family (say CatBoost or GradientBoosting from your comparison table),
* Design the exact `trainer.train_and_register_model()` signature and its MLflow logging,
* Then draft the FastAPI router for `/admin/train` and `/predict`.

[1]: https://medium.com/%40amirm.lavasani/how-to-structure-your-fastapi-projects-0219a6600a8f?utm_source=chatgpt.com "How to Structure Your FastAPI Projects"
[2]: https://www.datacamp.com/tutorial/pandas-profiling-ydata-profiling-in-python-guide?utm_source=chatgpt.com "Pandas Profiling (ydata-profiling) in Python: A Guide for ..."
[3]: https://tuanatran.medium.com/end-to-end-ml-deployment-with-mlflow-fastapi-and-aws-fargate-2fa554925792?utm_source=chatgpt.com "End-to-End ML Deployment with MLflow, FastAPI, and ..."
[4]: https://mlflow.org/docs/3.2.0/ml/deployment/?utm_source=chatgpt.com "MLflow Serving"
[5]: https://fastapi.tiangolo.com/deployment/docker/?utm_source=chatgpt.com "FastAPI in Containers - Docker"
[6]: https://github.com/99sbr/fastapi-template?utm_source=chatgpt.com "99sbr/fastapi-template"
